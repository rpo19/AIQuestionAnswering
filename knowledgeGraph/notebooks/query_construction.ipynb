{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import sparql\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "from nltk.corpus import stopwords\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = {\n",
    "    'p0': {'A': []},\n",
    "    'p1': {'A': ['B']},\n",
    "    'p2': {'A': ['B'],\n",
    "          'B': ['C']},\n",
    "    'p3': {'A': ['B'],\n",
    "          'C': ['B']},\n",
    "    'p4': {'A': ['B', 'C']},\n",
    "    'p5': {'A': ['B', 'C', 'D']},\n",
    "    'p6': {'A': ['B', 'C'],\n",
    "          'C': ['D']},\n",
    "    'p7': {'A': ['B'],\n",
    "          'B': ['C'],\n",
    "          'C': ['D']},\n",
    "    'p8': {'A': ['B'],\n",
    "          'B': ['C'],\n",
    "          'D': ['C']},\n",
    "    'p9': {'A': ['B'],\n",
    "          'B': ['C'],\n",
    "          'D': ['B']},\n",
    "    'p10': {'A': ['B'],\n",
    "           'B': ['C', 'D']},\n",
    "    'p11': {'A': ['B'],\n",
    "           'C': ['B'],\n",
    "           'D': ['B']}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-raise",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusions = ['<http://dbpedia.org/property/wikiPageUsesTemplate>',\n",
    "              '<http://dbpedia.org/ontology/wikiPageExternalLink>', \n",
    "              '<http://dbpedia.org/ontology/wikiPageID>', \n",
    "              '<http://dbpedia.org/ontology/wikiPageRevisionID>', \n",
    "              '<http://dbpedia.org/ontology/wikiPageLength>', \n",
    "              '<http://dbpedia.org/ontology/wikiPageWikiLink>', \n",
    "              '<http://www.w3.org/2000/01/rdf-schema#label>', \n",
    "              '<http://www.w3.org/2002/07/owl#sameAs>', \n",
    "              '<http://www.w3.org/1999/02/22-rdf-syntax-ns#type>', \n",
    "              '<http://schema.org/sameAs>', \n",
    "              '<http://purl.org/dc/terms/subject>', \n",
    "              '<http://xmlns.com/foaf/0.1/isPrimaryTopicOf>', \n",
    "              '<http://xmlns.com/foaf/0.1/depiction>', \n",
    "              '<http://www.w3.org/2000/01/rdf-schema#seeAlso>', \n",
    "              '<http://www.w3.org/2000/01/rdf-schema#comment>', \n",
    "              '<http://dbpedia.org/ontology/abstract>', \n",
    "              '<http://dbpedia.org/ontology/thumbnail>', \n",
    "              '<http://dbpedia.org/property/caption>', \n",
    "              '<http://dbpedia.org/property/captionAlign>', \n",
    "              '<http://dbpedia.org/property/image>', \n",
    "              '<http://dbpedia.org/property/imageFlag>', \n",
    "              '<http://www.w3.org/ns/prov#wasDerivedFrom>', \n",
    "              '<http://dbpedia.org/ontology/wikiPageRedirects>', \n",
    "              '<http://dbpedia.org/ontology/wikiPageDisambiguates>',\n",
    "             '<http://dbpedia.org/property/1namedata>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path='../../data/glove.twitter.27B.200d.txt'):\n",
    "        embeddings_dict = {}\n",
    "        print('Loading embeddings...')\n",
    "        with open(path, 'r', encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], \"float32\")\n",
    "                embeddings_dict[word] = vector\n",
    "        return embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-rings",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = load_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-nebraska",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryGraphBuilder():\n",
    "    def __init__(self, embeddings = None, bert_similarity = True):\n",
    "        if bert_similarity:\n",
    "            self.vectorizer = Vectorizer()\n",
    "        else:\n",
    "            if not embeddings:\n",
    "                self.embeddings = self.__load_embeddings()\n",
    "            else:\n",
    "                self.embeddings = embeddings\n",
    "        self.stops = stopwords.words('english')\n",
    "        self.exclusions = self.__get_exclusions()\n",
    "        self.bert_similarity = bert_similarity\n",
    "    \n",
    "    \"\"\"\n",
    "    Build query graph.\n",
    "    \n",
    "    :param question: natural language question\n",
    "    :param entity: entity resource\n",
    "    :param pattern: graph pattern of the question\n",
    "    \n",
    "    :return: query graph\n",
    "    \"\"\"\n",
    "    def build(self, question, entities, pattern):\n",
    "        # TODO: higher score linking\n",
    "        cn = [entities[0]]\n",
    "        # get pattern graph\n",
    "        p = self.__get_pattern(pattern)\n",
    "        # make a copy of the pattern\n",
    "        Q = p.copy()\n",
    "        # get non-intermediate nodes\n",
    "        NS = self.__get_non_intermediate_nodes(p)\n",
    "        \n",
    "        var_num = 0\n",
    "        \n",
    "        \n",
    "        while len(NS) != 0:\n",
    "            # empty relations set\n",
    "            R = pd.DataFrame(columns=['pred', 'label', 'direction'])\n",
    "            \n",
    "            # check if there nodes or edges unlabeled\n",
    "#             if self.__is_graph_labeled(Q):\n",
    "#                 return Q\n",
    "            \n",
    "            # check if last node\n",
    "            if len(NS) == 1:\n",
    "                edges = list(Q.out_edges(NS[0], data=True))\n",
    "                edges.extend(list(Q.in_edges(NS[0], data=True)))\n",
    "                \n",
    "                keep_going = True\n",
    "                \n",
    "                for edge in edges:\n",
    "                    if not edge[2]:\n",
    "                        keep_going = False\n",
    "                        break   \n",
    "                \n",
    "                if keep_going:\n",
    "                    unlabeled_node = NS[0]\n",
    "                    is_variable = True\n",
    "                    # check if cn URI is in q\n",
    "                    for c in cn:\n",
    "                        if c in entities and self.__check_entity_rel(c, r):\n",
    "                            Q.nodes[unlabeled_node]['label'] = c\n",
    "                            is_variable = False\n",
    "                            break\n",
    "\n",
    "                    if is_variable:\n",
    "                        # assemble variable in Q\n",
    "                        Q.nodes[unlabeled_node]['label'] = '?' + str(var_num)\n",
    "                        \n",
    "                    return Q\n",
    "                    \n",
    "            # check if NS has outgoing edges\n",
    "            if self.__check_if_outgoing(Q, NS, check_labels=True):\n",
    "                outgoing_relations = self.__get_relations(entities=cn, query_type='outgoing')\n",
    "                # add all outgoing relation found to R\n",
    "                R = R.append(outgoing_relations)\n",
    "                \n",
    "            # check if NS has incoming edges\n",
    "            if self.__check_if_incoming(Q, NS, check_labels=True):\n",
    "                outgoing_relations = self.__get_relations(entities=cn, query_type='incoming')\n",
    "                \n",
    "                # add all incoming relation found to R\n",
    "                R = R.append(outgoing_relations)\n",
    "\n",
    "            # get r, most relevant relation to question\n",
    "            if self.bert_similarity:\n",
    "                r = self.__get_most_relevant_relation_bert(question, R)\n",
    "            else:\n",
    "                r = self.__get_most_relevant_relation(question, R)\n",
    "            \n",
    "            # get r direction\n",
    "            r_direction = r.direction\n",
    "            \n",
    "            # get an unlabelled node in NS which has a relation respecting r direction\n",
    "            unlabeled_node = self.__get_unlabeled_node(Q, NS, r_direction)\n",
    "            \n",
    "            # get unlabeled edge from unlabeled_node\n",
    "            unlabeled_edge = self.__get_unlabeled_edge(Q, unlabeled_node, r_direction)\n",
    "            \n",
    "            is_variable = True\n",
    "            # check if cn URI is in q\n",
    "            for c in cn:\n",
    "                if c in entities and self.__check_entity_rel(c, r):\n",
    "                    Q.nodes[unlabeled_node]['label'] = c\n",
    "                    is_variable = False\n",
    "                    break\n",
    "                    \n",
    "            if is_variable:\n",
    "                # assemble variable in Q\n",
    "                Q.nodes[unlabeled_node]['label'] = '?' + str(var_num)\n",
    "            \n",
    "            # assemble relation in Q\n",
    "            Q[unlabeled_edge[0]][unlabeled_edge[1]]['label'] = r.pred\n",
    "            # for drawing purposes\n",
    "            Q[unlabeled_edge[0]][unlabeled_edge[1]]['short_label'] = r.label\n",
    "            \n",
    "            # get adjacent unexplored node\n",
    "            NS = self.__get_adjacent_unexplored(Q)\n",
    "            # get entities corresponding to NS\n",
    "            cn = self.__get_candidate_entities(cn, r)\n",
    "        return    \n",
    "    \n",
    "    def draw_graph(self, Q):\n",
    "        labels_nodes = nx.get_node_attributes(Q, 'label') \n",
    "        labels_edges = nx.get_edge_attributes(Q, 'short_label')\n",
    "        pos = nx.spring_layout(Q)\n",
    "        nx.draw(Q, pos, labels=labels_nodes)\n",
    "        nx.draw_networkx_edge_labels(Q, pos ,edge_labels=labels_edges)\n",
    "        \n",
    "    \"\"\"\n",
    "    Get graph pattern for a pattern p.\n",
    "    \n",
    "    :param pattern: pattern dictionary\n",
    "    \n",
    "    :return: networkx graph of the pattern p\n",
    "    \"\"\"\n",
    "    def __get_pattern(self, pattern):\n",
    "        return nx.from_dict_of_lists(patterns[pattern], \n",
    "                                     create_using=nx.DiGraph)      \n",
    "    \n",
    "    \"\"\"\n",
    "    Get non intermediate notes for a graph pattern p.\n",
    "    \n",
    "    :param p: graph pattern\n",
    "    \n",
    "    :return: dict of non-intermediary nodes\n",
    "    \"\"\"\n",
    "#     def __get_non_intermediate_nodes(self, p):\n",
    "#         return {node: {'in_degree': p.in_degree(node), 'out_degree': p.out_degree(node)} \n",
    "#                          for node in p.nodes if p.out_degree(node) + p.in_degree(node) < 2}\n",
    "    def __get_non_intermediate_nodes(self, p):\n",
    "        return [node for node in p.nodes if p.out_degree(node) + p.in_degree(node) < 2]\n",
    "    \n",
    "    \"\"\"\n",
    "    Check if graph has unlabeled relations.\n",
    "    \n",
    "    :param Q: graph\n",
    "    \n",
    "    :return: True if completaly labelled, False otherwise\n",
    "    \"\"\"\n",
    "    def __is_graph_labeled(self, Q):\n",
    "        # check if nodes are labeled\n",
    "        for node in Q.nodes:\n",
    "            if not Q.nodes[node]:\n",
    "                return False\n",
    "        # check if edges are labeled\n",
    "        for _,_,e in Q.edges(data=True):\n",
    "            if not e:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def __is_labeled(self, graph_pattern, node):\n",
    "        return False if not graph_pattern.nodes[node] else True\n",
    "    \n",
    "    def __get_unlabeled_node(self, graph_pattern, NS, direction):\n",
    "        for node in NS:\n",
    "            if direction == 'outgoing':\n",
    "                if (not self.__is_labeled(graph_pattern, node)) and self.__check_if_outgoing(graph_pattern, [node]):\n",
    "                        return node\n",
    "            else:\n",
    "                if (not self.__is_labeled(graph_pattern, node)) and self.__check_if_incoming(graph_pattern, [node]):\n",
    "                        return node\n",
    "        return None\n",
    "    \n",
    "    def __get_unlabeled_edge(self, graph_pattern, node, direction):\n",
    "        if direction == 'outgoing':\n",
    "            edges = list(graph_pattern.out_edges(node, data=True))\n",
    "        else:\n",
    "            edges = list(graph_pattern.in_edges(node, data=True))\n",
    "\n",
    "        for edge in edges:\n",
    "            if not edge[2]:\n",
    "                return (edge[0], edge[1])\n",
    "        return None\n",
    "                    \n",
    "    \n",
    "    \"\"\"\n",
    "    Check if nodes have outgoing relations.\n",
    "    \n",
    "    :param NS: dict of nodes (see __get_non_intermediate_nodes(p))\n",
    "    \n",
    "    :return: True if they have outgoing relations, False otherwise\n",
    "    \"\"\"\n",
    "#     def __check_if_outgoing(self, NS):\n",
    "#         out_degree = [NS[node]['out_degree'] for node in NS]\n",
    "#         return max(out_degree) > 0\n",
    "    def __check_if_outgoing(self, graph_pattern, NS, check_labels=False):\n",
    "        for node in NS:\n",
    "            if graph_pattern.out_degree(node) > 0:\n",
    "                if check_labels:\n",
    "                    edges = list(graph_pattern.out_edges(node, data=True))\n",
    "\n",
    "                    for edge in edges:\n",
    "                        if not edge[2]:\n",
    "                            return True\n",
    "                else:\n",
    "                    return True\n",
    "\n",
    "        return False\n",
    "    \n",
    "    \"\"\"\n",
    "    Check if nodes have incoming relations.\n",
    "    \n",
    "    :param NS: dict of nodes (see __get_non_intermediate_nodes(p))\n",
    "    \n",
    "    :return: True if they have incoming relations, False otherwise\n",
    "    \"\"\"\n",
    "#     def __check_if_incoming(self, NS):\n",
    "#         in_degree = [NS[node]['in_degree'] for node in NS]\n",
    "#         return max(in_degree) > 0\n",
    "    def __check_if_incoming(self, graph_pattern, NS, check_labels=False):\n",
    "        for node in NS:\n",
    "            if graph_pattern.in_degree(node) > 0:\n",
    "                if check_labels:\n",
    "                    edges = list(graph_pattern.in_edges(node, data=True))\n",
    "\n",
    "                    for edge in edges:\n",
    "                        if not edge[2]:\n",
    "                            return True\n",
    "                else:\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "    \"\"\"\n",
    "    Get outgoing or incoming relations for an entity.\n",
    "    \n",
    "    :param entity: entity for which you want to find relations\n",
    "    :param query_type: 'outgoing' for outgoing relations, 'incoming' for incoming relations\n",
    "    :param query_type: SPARQL endpoint\n",
    "    \n",
    "    :return: dataframe of outgoing/incoming relations (URI, label)\n",
    "    \"\"\"\n",
    "    def __get_relations(self, entities, query_type, endpoint = 'http://dbpedia.org/sparql'):\n",
    "        #print(query_type)\n",
    "        q = self.__get_query(entities, query_type)\n",
    "        results = sparql.query(endpoint, q)\n",
    "        \n",
    "        relations = pd.DataFrame(columns=['pred', 'label', 'direction'])\n",
    "        for i, row in enumerate(results):\n",
    "            (pred, label) = sparql.unpack_row(row)\n",
    "\n",
    "            if not label:\n",
    "                label = self.__parse_predicate(pred)\n",
    "            else:\n",
    "                label = label.replace('-', ' ')\n",
    "                \n",
    "            tmp = relations[relations.label == label]\n",
    "            \n",
    "            # we keep dbo predicates if multiple with the same label\n",
    "            if not tmp.empty and \"http://dbpedia.org/ontology/\" in pred:\n",
    "                relations.loc[tmp.index, ['pred']] = pred\n",
    "            else:\n",
    "                relations = relations.append({\n",
    "                    'pred': pred,\n",
    "                    'label': label.lower(),\n",
    "                    'direction': query_type\n",
    "                }, ignore_index=True)\n",
    "    \n",
    "        return relations\n",
    "    \n",
    "    \"\"\"\n",
    "    Get predicates to exclude from the query.\n",
    "    \n",
    "    :return: concatenation of all exclusions\n",
    "    \"\"\"\n",
    "    def __get_exclusions(self):\n",
    "        return ', '.join(exclusions)\n",
    "    \n",
    "    \"\"\"\n",
    "    Parse URI to extract a label.\n",
    "    \n",
    "    :param pred: predicate URI\n",
    "    \n",
    "    :return: predicate label\n",
    "    \"\"\"\n",
    "    def __parse_predicate(self, pred):\n",
    "        last = pred.rsplit('/',1)[1]\n",
    "        splitted = re.findall(r'[A-Z]?[a-z]+|[A-Z]+(?=[A-Z]|$)', last) \n",
    "        return ' '.join(splitted)\n",
    "    \n",
    "    \"\"\"\n",
    "    Get SPARQL query to get incoming or outgoing relations for an entity.\n",
    "    \n",
    "    :param entity: entity for which you want to find relations\n",
    "    :param query_type: 'outgoing' for outgoing relations query, 'incoming' for incoming relations query\n",
    "    \n",
    "    :return: SPARQL query\n",
    "    \"\"\"\n",
    "    def __get_query(self, entities, query_type):\n",
    "        if query_type == 'outgoing':\n",
    "            body = ''\n",
    "            first = True\n",
    "            for entity in entities:\n",
    "                if not first: \n",
    "                    body += \"UNION \"\n",
    "                    first = False\n",
    "                    \n",
    "                body += \"{ \\\n",
    "                        <\"+ entity +\"> ?pred ?obj.  \\\n",
    "                        FILTER (lang(?pred_label) = 'en').  \\\n",
    "                        OPTIONAL {  \\\n",
    "                            ?pred rdfs:label ?pred_label . \\\n",
    "                            BIND (STR(?pred_label)  AS ?pred_label_stripped). \\\n",
    "                        } . \\\n",
    "                        FILTER(?pred NOT IN (\"+ self.exclusions +\") ). } \"\n",
    "                \n",
    "            return \"select distinct ?pred ?pred_label_stripped where { \"+ body +\" }\"\n",
    "      \n",
    "        elif query_type == 'incoming':\n",
    "            body = ''\n",
    "            first = True\n",
    "            for entity in entities:\n",
    "                if not first: \n",
    "                    body += \"UNION \"\n",
    "                    first = False\n",
    "                    \n",
    "                body += \"{ \\\n",
    "                        ?subj ?pred <\" + entity + \"> .  \\\n",
    "                        FILTER (lang(?pred_label) = 'en').  \\\n",
    "                        OPTIONAL {  \\\n",
    "                            ?pred rdfs:label ?pred_label . \\\n",
    "                            BIND (STR(?pred_label)  AS ?pred_label_stripped). \\\n",
    "                        } . \\\n",
    "                        FILTER(?pred NOT IN (\"+ self.exclusions +\") ). } \"\n",
    "                \n",
    "            return \"select distinct ?pred ?pred_label_stripped where { \"+ body +\" }\"\n",
    "\n",
    "        else:\n",
    "            raise ValueError('query_type has to be either \\'incoming\\' or \\'outgoing\\' for value:' + query_type)\n",
    "            \n",
    "    def __check_entity_rel(self, entity, relation, endpoint = 'http://dbpedia.org/sparql'):\n",
    "        if relation.direction == 'outgoing':\n",
    "            q = f\"\"\"\n",
    "                ASK WHERE {{\n",
    "                <{entity}> <{relation.pred}> ?_ .\n",
    "                }}\n",
    "                \"\"\"\n",
    "        else:\n",
    "            q = f\"\"\"\n",
    "                ASK WHERE {{\n",
    "                ?_ <{relation.pred}> <{entity}> .\n",
    "                }}\n",
    "                \"\"\"\n",
    "        results = sparql.query(endpoint, q)\n",
    "        return results.hasresult()\n",
    "    \n",
    "    \n",
    "    def __get_candidate_entities(self, cn, relation, endpoint = 'http://dbpedia.org/sparql'):\n",
    "        q = \"\"\n",
    "        body = \"\"\n",
    "        first = True\n",
    "        if relation.direction == 'outgoing':\n",
    "            for entity in cn:\n",
    "                if not first: \n",
    "                    body += \"UNION \"\n",
    "                    first = False\n",
    "                    \n",
    "                body += f\"\"\"<{entity}> <{relation.pred}> ?obj .\"\"\"\n",
    "                \n",
    "            q = \"select distinct ?obj where { \"+ body +\" }\"\n",
    "        else:\n",
    "            for entity in cn:\n",
    "                if not first: \n",
    "                    body += \"UNION \"\n",
    "                    first = False\n",
    "                    \n",
    "                body += f\"\"\"?sbj <{relation.pred}> <{entity}> .\"\"\"\n",
    "                \n",
    "            q = \"select distinct ?sbj where { \"+ body +\" }\"\n",
    "        \n",
    "        results = sparql.query(endpoint, q)\n",
    "        \n",
    "        candidate_entities = []\n",
    "        for row in results:\n",
    "            candidate_entity = sparql.unpack_row(row)\n",
    "            candidate_entities.append(candidate_entity[0])\n",
    "        \n",
    "        return candidate_entities\n",
    "            \n",
    "    \n",
    "    \"\"\"\n",
    "    Load Glove embeddings.\n",
    "    \n",
    "    :param path: path to glove emeddings\n",
    "    \n",
    "    :return: dictionary containing embeddings for each word\n",
    "    \"\"\"\n",
    "    def __load_embeddings(self, path='../../data/glove.twitter.27B.200d.txt'):\n",
    "        embeddings_dict = {}\n",
    "        print('Loading embeddings...')\n",
    "        with open(path, 'r', encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], \"float32\")\n",
    "                embeddings_dict[word] = vector\n",
    "        return embeddings_dict\n",
    "    \n",
    "    \"\"\"\n",
    "    Get most relevant relation using given embedding and levenshtein_distance.\n",
    "    \n",
    "    :param question: question in natural language\n",
    "    :param R: set of candidate relations\n",
    "    :param lambda_param: hyperparameter describing the importance of cosine similarity and levenshtein_distance\n",
    "    \n",
    "    :return: label of most relevant relation\n",
    "    \"\"\"\n",
    "    def __get_most_relevant_relation(self, question, R, lambda_param=0.5):\n",
    "        unique_relations = R\n",
    "        question = question.lower().replace('?', ' ?')\n",
    "        # tokenize question\n",
    "        question_tokens = question.split()\n",
    "        # remove stopwords and punctuation tokens\n",
    "        question_tokens = [token for token in question_tokens \n",
    "                               if token not in self.stops and token not in string.punctuation]\n",
    "        \n",
    "        relevances = []\n",
    "        \n",
    "        for index, row in unique_relations.iterrows():\n",
    "            # tokenize label\n",
    "            relation_tokens = row['label'].split()\n",
    "            \n",
    "            relevance = 0\n",
    "            for rel_token in relation_tokens:                \n",
    "                \n",
    "                for question_token in question_tokens:\n",
    "                    \n",
    "                    if rel_token in self.embeddings and question_token in self.embeddings:\n",
    "                        \n",
    "                        rel_token_embedding = self.embeddings[rel_token]\n",
    "                        question_token_embedding = self.embeddings[question_token]\n",
    "                        \n",
    "                        # compute cosine similarity\n",
    "                        cos_sim = 1 - spatial.distance.cosine(rel_token_embedding, question_token_embedding)\n",
    "                    else:\n",
    "                        cos_sim = 0\n",
    "                    # compute lev distance\n",
    "                    lev_distance = levenshtein_distance(question_token, rel_token)\n",
    "                    # sum to previous relenvances of relation tokens and question tokens\n",
    "                    relevance += lambda_param * cos_sim + (1 - lambda_param) * 1/(lev_distance+1)\n",
    "            \n",
    "            relevances.append(relevance/len(relation_tokens))\n",
    "        relevances = np.array(relevances)\n",
    "        \n",
    "        return unique_relations.iloc[np.argmax(relevances)]\n",
    "    \n",
    "    \"\"\"\n",
    "    Get most relevant relation using bert and levenshtein_distance.\n",
    "    \n",
    "    :param question: question in natural language\n",
    "    :param R: set of candidate relations\n",
    "    :param lambda_param: hyperparameter describing the importance of cosine similarity and levenshtein_distance\n",
    "    \n",
    "    :return: label of most relevant relation\n",
    "    \"\"\"\n",
    "    def __get_most_relevant_relation_bert(self, question, R, lambda_param=0.4):\n",
    "        unique_relations = R\n",
    "        \n",
    "        question_processed = question.lower().replace('?', ' ?')\n",
    "        # tokenize question\n",
    "        question_tokens = question.split()\n",
    "        question_tokens = [token for token in question_tokens \n",
    "                               if token not in self.stops and token not in string.punctuation]\n",
    "        \n",
    "        relevances = []\n",
    "        \n",
    "        # generate sentence embeddings for question\n",
    "        self.vectorizer.bert([question])\n",
    "        embedding_question = self.vectorizer.vectors\n",
    "        # generate sentence embeddings for relations\n",
    "        self.vectorizer.bert(unique_relations['label'].values)\n",
    "        embeddings_relations = self.vectorizer.vectors\n",
    "        \n",
    "        for i, rel_embedding in enumerate(embeddings_relations):\n",
    "            cos_sim = 1 - spatial.distance.cosine(embedding_question, rel_embedding)\n",
    "            \n",
    "            relation_tokens = unique_relations.iloc[i]['label'].split()\n",
    "            \n",
    "            lev = 0\n",
    "            for rel_token in relation_tokens:                \n",
    "                \n",
    "                for question_token in question_tokens:\n",
    "                    # compute lev distance\n",
    "                    lev_distance = levenshtein_distance(question_token, rel_token)\n",
    "                    # sum to previous relenvances of relation tokens and question tokens\n",
    "                    lev += 1/(lev_distance+1)\n",
    "            \n",
    "            relevance = lambda_param * cos_sim + (1 - lambda_param) * (lev / len(relation_tokens))\n",
    "            relevances.append(relevance)\n",
    "        relevances = np.array(relevances)\n",
    "\n",
    "        return unique_relations.iloc[np.argmax(relevances)]\n",
    "    \n",
    "    def __get_adjacent_unexplored(self, Q):\n",
    "        neighbors = set()\n",
    "        for node in Q.nodes:\n",
    "            if Q.nodes[node]:\n",
    "                for neighbor in Q.neighbors(node):\n",
    "                    if not Q.nodes[neighbor]:\n",
    "                        neighbors.add(neighbor)\n",
    "        return list(neighbors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-inventory",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_builder = QueryGraphBuilder(embeddings = embeddings, bert_similarity = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-scanning",
   "metadata": {},
   "outputs": [],
   "source": [
    "grafo.in_edges(\"B\", data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-pressure",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Q = query_builder.build(question='What is the name of the spouse of Barack Obama?', entities=[\"http://dbpedia.org/resource/Barack_Obama\"], pattern='p2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_builder.draw_graph(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-greenhouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.nodes(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-louisville",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
