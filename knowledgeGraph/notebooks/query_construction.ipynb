{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import sparql\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "from nltk.corpus import stopwords\n",
    "from sent2vec.vectorizer import Vectorizer\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = {\n",
    "    'p0': {'A': []},\n",
    "    'p1': {'A': ['B']},\n",
    "    'p2': {'A': ['B'],\n",
    "          'B': ['C']},\n",
    "    'p3': {'A': ['B'],\n",
    "          'C': ['B']},\n",
    "    'p4': {'A': ['B', 'C']},\n",
    "    'p5': {'A': ['B', 'C', 'D']},\n",
    "    'p6': {'A': ['B', 'C'],\n",
    "          'C': ['D']},\n",
    "    'p7': {'A': ['B'],\n",
    "          'B': ['C'],\n",
    "          'C': ['D']},\n",
    "    'p8': {'A': ['B'],\n",
    "          'B': ['C'],\n",
    "          'D': ['C']},\n",
    "    'p9': {'A': ['B'],\n",
    "          'B': ['C'],\n",
    "          'D': ['B']},\n",
    "    'p10': {'A': ['B'],\n",
    "           'B': ['C', 'D']},\n",
    "    'p11': {'A': ['B'],\n",
    "           'C': ['B'],\n",
    "           'D': ['B']}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-raise",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusions = ['<http://dbpedia.org/property/wikiPageUsesTemplate>',\n",
    "              '<http://dbpedia.org/ontology/wikiPageExternalLink>', \n",
    "              '<http://dbpedia.org/ontology/wikiPageID>', \n",
    "              '<http://dbpedia.org/ontology/wikiPageRevisionID>', \n",
    "              '<http://dbpedia.org/ontology/wikiPageLength>', \n",
    "              '<http://dbpedia.org/ontology/wikiPageWikiLink>', \n",
    "              '<http://www.w3.org/2000/01/rdf-schema#label>', \n",
    "              '<http://www.w3.org/2002/07/owl#sameAs>', \n",
    "              '<http://www.w3.org/1999/02/22-rdf-syntax-ns#type>', \n",
    "              '<http://schema.org/sameAs>', \n",
    "              '<http://purl.org/dc/terms/subject>', \n",
    "              '<http://xmlns.com/foaf/0.1/isPrimaryTopicOf>', \n",
    "              '<http://xmlns.com/foaf/0.1/depiction>', \n",
    "              '<http://www.w3.org/2000/01/rdf-schema#seeAlso>', \n",
    "              '<http://www.w3.org/2000/01/rdf-schema#comment>', \n",
    "              '<http://dbpedia.org/ontology/abstract>', \n",
    "              '<http://dbpedia.org/ontology/thumbnail>', \n",
    "              '<http://dbpedia.org/property/caption>', \n",
    "              '<http://dbpedia.org/property/captionAlign>', \n",
    "              '<http://dbpedia.org/property/image>', \n",
    "              '<http://dbpedia.org/property/imageFlag>', \n",
    "              '<http://www.w3.org/ns/prov#wasDerivedFrom>', \n",
    "              '<http://dbpedia.org/ontology/wikiPageRedirects>', \n",
    "              '<http://dbpedia.org/ontology/wikiPageDisambiguates>',\n",
    "             '<http://dbpedia.org/property/1namedata>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path='../../data/glove.twitter.27B.200d.txt'):\n",
    "        embeddings_dict = {}\n",
    "        print('Loading embeddings...')\n",
    "        with open(path, 'r', encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], \"float32\")\n",
    "                embeddings_dict[word] = vector\n",
    "        return embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-rings",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = load_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-nebraska",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryBuilder():\n",
    "    def __init__(self, embeddings = None, bert_similarity = True):\n",
    "        if bert_similarity:\n",
    "            self.vectorizer = Vectorizer()\n",
    "        else:\n",
    "            if not embeddings:\n",
    "                self.embeddings = self.__load_embeddings()\n",
    "            else:\n",
    "                self.embeddings = embeddings\n",
    "        self.stops = stopwords.words('english')\n",
    "        self.exclusions = self.__get_exclusions()\n",
    "        self.bert_similarity = bert_similarity\n",
    "    \n",
    "    \"\"\"\n",
    "    Build query graph.\n",
    "    \n",
    "    :param question: natural language question\n",
    "    :param entity: entity resource\n",
    "    :param pattern: graph pattern of the question\n",
    "    \n",
    "    :return: query graph\n",
    "    \"\"\"\n",
    "    def build(self, question, entity, pattern):\n",
    "        # TODO: call DBPedia entity extractor/linker, remove entity param\n",
    "        entities = [entity]\n",
    "        # TODO: higher score linking\n",
    "        cn = entities[0]\n",
    "        # get pattern graph\n",
    "        p = self.__get_pattern(pattern)\n",
    "        # make a copy of the pattern\n",
    "        Q = p.copy()\n",
    "        # get non-intermediate nodes\n",
    "        NS = self.__get_non_intermediate_nodes(p)\n",
    "        \n",
    "        while True:\n",
    "            # empty relations set\n",
    "            R = pd.DataFrame(columns=['pred', 'label', 'direction'])\n",
    "            \n",
    "            # check if there nodes or edges unlabeled\n",
    "            if self.__is_labeled(Q):\n",
    "                return Q\n",
    "            \n",
    "            # check if NS has outgoing edges\n",
    "            if self.__check_if_outgoing(NS):\n",
    "                outgoing_relations = self.__get_relations(entity=cn, query_type='outgoing')\n",
    "                # add all outgoing relation found to R\n",
    "                R = R.append(outgoing_relations)\n",
    "                \n",
    "            # check if NS has incoming edges\n",
    "            if self.__check_if_incoming(NS):\n",
    "                outgoing_relations = self.__get_relations(entity=cn, query_type='incoming')\n",
    "                \n",
    "                # add all incoming relation found to R\n",
    "                R = R.append(outgoing_relations)\n",
    "            \n",
    "            # get r, most relevant relation to question\n",
    "            if self.bert_similarity:\n",
    "                r = self.__get_most_relevant_relation_bert(question, R)\n",
    "            else:\n",
    "                r = self.__get_most_relevant_relation(question, R)\n",
    "            \n",
    "            \n",
    "            # check if cn URI is in q\n",
    "            if cn in entities:\n",
    "                # assemble entity and relation r in Q\n",
    "                # TODO TODO\n",
    "                \n",
    "            else:\n",
    "                return\n",
    "                # assemble variable and relation r in Q\n",
    "            \n",
    "            # NS = adiacent node to explored structure\n",
    "            # cn = not sure\n",
    "        return    \n",
    "        \n",
    "    \"\"\"\n",
    "    Get graph pattern for a pattern p.\n",
    "    \n",
    "    :param pattern: pattern dictionary\n",
    "    \n",
    "    :return: networkx graph of the pattern p\n",
    "    \"\"\"\n",
    "    def __get_pattern(self, pattern):\n",
    "        return nx.from_dict_of_lists(patterns[pattern], \n",
    "                                     create_using=nx.DiGraph)      \n",
    "    \n",
    "    \"\"\"\n",
    "    Get non intermediate notes for a graph pattern p.\n",
    "    \n",
    "    :param p: graph pattern\n",
    "    \n",
    "    :return: dict of non-intermediary nodes\n",
    "    \"\"\"\n",
    "    def __get_non_intermediate_nodes(self, p):\n",
    "        return {node: {'in_degree': p.in_degree(node), 'out_degree': p.out_degree(node)} \n",
    "                         for node in p.nodes if p.out_degree(node) + p.in_degree(node) < 2}\n",
    "    \n",
    "    \"\"\"\n",
    "    Check if graph has unlabeled relations.\n",
    "    \n",
    "    :param Q: graph\n",
    "    \n",
    "    :return: True if completaly labelled, False otherwise\n",
    "    \"\"\"\n",
    "    def __is_labeled(self, Q):\n",
    "        # check if nodes are labeled\n",
    "        for node in Q.nodes:\n",
    "            if not Q.nodes[node]:\n",
    "                return False\n",
    "        # check if edges are labeled\n",
    "        for _,_,e in Q.edges(data=True):\n",
    "            if not e:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    \"\"\"\n",
    "    Check if nodes have outgoing relations.\n",
    "    \n",
    "    :param NS: dict of nodes (see __get_non_intermediate_nodes(p))\n",
    "    \n",
    "    :return: True if they have outgoing relations, False otherwise\n",
    "    \"\"\"\n",
    "    def __check_if_outgoing(self, NS):\n",
    "        out_degree = [NS[node]['out_degree'] for node in NS]\n",
    "        return max(out_degree) > 0\n",
    "    \n",
    "    \"\"\"\n",
    "    Check if nodes have incoming relations.\n",
    "    \n",
    "    :param NS: dict of nodes (see __get_non_intermediate_nodes(p))\n",
    "    \n",
    "    :return: True if they have incoming relations, False otherwise\n",
    "    \"\"\"\n",
    "    def __check_if_incoming(self, NS):\n",
    "        in_degree = [NS[node]['in_degree'] for node in NS]\n",
    "        return max(in_degree) > 0\n",
    "    \n",
    "    \"\"\"\n",
    "    Get outgoing or incoming relations for an entity.\n",
    "    \n",
    "    :param entity: entity for which you want to find relations\n",
    "    :param query_type: 'outgoing' for outgoing relations, 'incoming' for incoming relations\n",
    "    :param query_type: SPARQL endpoint\n",
    "    \n",
    "    :return: dataframe of outgoing/incoming relations (URI, label)\n",
    "    \"\"\"\n",
    "    def __get_relations(self, entity, query_type, endpoint = 'http://dbpedia.org/sparql'):\n",
    "        print(query_type)\n",
    "        q = self.__get_query(entity, query_type)\n",
    "        results = sparql.query(endpoint, q)\n",
    "        \n",
    "        relations = pd.DataFrame(columns=['pred', 'label', 'direction'])\n",
    "        for i, row in enumerate(results):\n",
    "            (pred, label) = sparql.unpack_row(row)\n",
    "            \n",
    "            if not label:\n",
    "                label = self.__parse_predicate(pred)\n",
    "            else:\n",
    "                label = label.replace('-', ' ')\n",
    "                \n",
    "            tmp = relations[relations.label == label]\n",
    "            \n",
    "            # we keep dbo predicates if multiple with the same label\n",
    "            if not tmp.empty and \"http://dbpedia.org/ontology/\" in pred:\n",
    "                relations.loc[tmp.index, ['pred']] = pred\n",
    "            else:\n",
    "                relations = relations.append({\n",
    "                    'pred': pred,\n",
    "                    'label': label.lower(),\n",
    "                    'direction': query_type\n",
    "                }, ignore_index=True)\n",
    "    \n",
    "        return relations\n",
    "    \n",
    "    \"\"\"\n",
    "    Get predicates to exclude from the query.\n",
    "    \n",
    "    :return: concatenation of all exclusions\n",
    "    \"\"\"\n",
    "    def __get_exclusions(self):\n",
    "        return ', '.join(exclusions)\n",
    "    \n",
    "    \"\"\"\n",
    "    Parse URI to extract a label.\n",
    "    \n",
    "    :param pred: predicate URI\n",
    "    \n",
    "    :return: predicate label\n",
    "    \"\"\"\n",
    "    def __parse_predicate(self, pred):\n",
    "        last = pred.rsplit('/',1)[1]\n",
    "        splitted = re.findall(r'[A-Z]?[a-z]+|[A-Z]+(?=[A-Z]|$)', last) \n",
    "        return ' '.join(splitted)\n",
    "    \n",
    "    \"\"\"\n",
    "    Get SPARQL query to get incoming or outgoing relations for an entity.\n",
    "    \n",
    "    :param entity: entity for which you want to find relations\n",
    "    :param query_type: 'outgoing' for outgoing relations query, 'incoming' for incoming relations query\n",
    "    \n",
    "    :return: SPARQL query\n",
    "    \"\"\"\n",
    "    def __get_query(self, entity, query_type):\n",
    "        if query_type == 'outgoing':\n",
    "            return \"select distinct ?pred ?pred_label_stripped \\\n",
    "                    where {  \\\n",
    "                        \"+ entity +\" ?pred ?obj.  \\\n",
    "                        FILTER (lang(?pred_label) = 'en').  \\\n",
    "                        OPTIONAL {  \\\n",
    "                            ?pred rdfs:label ?pred_label . \\\n",
    "                            BIND (STR(?pred_label)  AS ?pred_label_stripped). \\\n",
    "                        } . \\\n",
    "                        FILTER(?pred NOT IN (\"+ self.exclusions +\") ). \\\n",
    "                    }\"\n",
    "        elif query_type == 'incoming':\n",
    "            return \"select distinct ?pred ?pred_label_stripped \\\n",
    "                    where { \\\n",
    "                        ?subj ?pred \" + entity + \". \\\n",
    "                        ?pred rdfs:label ?pred_label. \\\n",
    "                        FILTER (lang(?pred_label) = 'en').  \\\n",
    "                        OPTIONAL {  \\\n",
    "                            ?pred rdfs:label ?pred_label . \\\n",
    "                            BIND (STR(?pred_label)  AS ?pred_label_stripped). \\\n",
    "                        } . \\\n",
    "                        FILTER(?pred NOT IN (\" + self.exclusions + \") ). }\"\n",
    "\n",
    "        else:\n",
    "            raise ValueError('query_type has to be either \\'incoming\\' or \\'outgoing\\' for value:' + query_type)\n",
    "    \n",
    "    \"\"\"\n",
    "    Load Glove embeddings.\n",
    "    \n",
    "    :param path: path to glove emeddings\n",
    "    \n",
    "    :return: dictionary containing embeddings for each word\n",
    "    \"\"\"\n",
    "    def __load_embeddings(self, path='../../data/glove.twitter.27B.200d.txt'):\n",
    "        embeddings_dict = {}\n",
    "        print('Loading embeddings...')\n",
    "        with open(path, 'r', encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], \"float32\")\n",
    "                embeddings_dict[word] = vector\n",
    "        return embeddings_dict\n",
    "    \n",
    "    \"\"\"\n",
    "    Get most relevant relation using given embedding and levenshtein_distance.\n",
    "    \n",
    "    :param question: question in natural language\n",
    "    :param R: set of candidate relations\n",
    "    :param lambda_param: hyperparameter describing the importance of cosine similarity and levenshtein_distance\n",
    "    \n",
    "    :return: label of most relevant relation\n",
    "    \"\"\"\n",
    "    def __get_most_relevant_relation(self, question, R, lambda_param=0.5):\n",
    "        unique_relations = R\n",
    "        question = question.lower().replace('?', ' ?')\n",
    "        # tokenize question\n",
    "        question_tokens = question.split()\n",
    "        # remove stopwords and punctuation tokens\n",
    "        question_tokens = [token for token in question_tokens \n",
    "                               if token not in self.stops and token not in string.punctuation]\n",
    "        \n",
    "        relevances = []\n",
    "        \n",
    "        for index, row in unique_relations.iterrows():\n",
    "            # tokenize label\n",
    "            relation_tokens = row['label'].split()\n",
    "            \n",
    "            relevance = 0\n",
    "            for rel_token in relation_tokens:                \n",
    "                \n",
    "                for question_token in question_tokens:\n",
    "                    \n",
    "                    if rel_token in self.embeddings and question_token in self.embeddings:\n",
    "                        \n",
    "                        rel_token_embedding = self.embeddings[rel_token]\n",
    "                        question_token_embedding = self.embeddings[question_token]\n",
    "                        \n",
    "                        # compute cosine similarity\n",
    "                        cos_sim = 1 - spatial.distance.cosine(rel_token_embedding, question_token_embedding)\n",
    "                    else:\n",
    "                        cos_sim = 0\n",
    "                    # compute lev distance\n",
    "                    lev_distance = levenshtein_distance(question_token, rel_token)\n",
    "                    # sum to previous relenvances of relation tokens and question tokens\n",
    "                    relevance += lambda_param * cos_sim + (1 - lambda_param) * 1/(lev_distance+1)\n",
    "            \n",
    "            relevances.append(relevance/len(relation_tokens))\n",
    "        relevances = np.array(relevances)\n",
    "        \n",
    "        return unique_relations.iloc[np.argmax(relevances)]\n",
    "    \n",
    "    \"\"\"\n",
    "    Get most relevant relation using bert and levenshtein_distance.\n",
    "    \n",
    "    :param question: question in natural language\n",
    "    :param R: set of candidate relations\n",
    "    :param lambda_param: hyperparameter describing the importance of cosine similarity and levenshtein_distance\n",
    "    \n",
    "    :return: label of most relevant relation\n",
    "    \"\"\"\n",
    "    def __get_most_relevant_relation_bert(self, question, R, lambda_param=0.4):\n",
    "        unique_relations = R\n",
    "        \n",
    "        question_processed = question.lower().replace('?', ' ?')\n",
    "        # tokenize question\n",
    "        question_tokens = question.split()\n",
    "        question_tokens = [token for token in question_tokens \n",
    "                               if token not in self.stops and token not in string.punctuation]\n",
    "        \n",
    "        relevances = []\n",
    "        \n",
    "        # generate sentence embeddings for question\n",
    "        self.vectorizer.bert([question])\n",
    "        embedding_question = self.vectorizer.vectors\n",
    "        # generate sentence embeddings for relations\n",
    "        self.vectorizer.bert(unique_relations['label'].values)\n",
    "        embeddings_relations = self.vectorizer.vectors\n",
    "        \n",
    "        for i, rel_embedding in enumerate(embeddings_relations):\n",
    "            cos_sim = 1 - spatial.distance.cosine(embedding_question, rel_embedding)\n",
    "            \n",
    "            relation_tokens = unique_relations.iloc[i]['label'].split()\n",
    "            \n",
    "            lev = 0\n",
    "            for rel_token in relation_tokens:                \n",
    "                \n",
    "                for question_token in question_tokens:\n",
    "                    # compute lev distance\n",
    "                    lev_distance = levenshtein_distance(question_token, rel_token)\n",
    "                    # sum to previous relenvances of relation tokens and question tokens\n",
    "                    lev += 1/(lev_distance+1)\n",
    "            \n",
    "            relevance = lambda_param * cos_sim + (1 - lambda_param) * (lev / len(relation_tokens))\n",
    "            relevances.append(relevance)\n",
    "        relevances = np.array(relevances)\n",
    "\n",
    "        return unique_relations.iloc[np.argmax(relevances)]\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-inventory",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_builder = QueryBuilder(embeddings = embeddings, bert_similarity = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-pressure",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = query_builder.build(question='Who is the spouse of Barack Obama?', entity=\"dbr:Barack_Obama\", pattern='p1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-present",
   "metadata": {},
   "outputs": [],
   "source": [
    "prova = pd.DataFrame()\n",
    "prova['relation'] = a\n",
    "prova['score'] = b\n",
    "prova.sort_values('score',ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-tonight",
   "metadata": {},
   "outputs": [],
   "source": [
    "'dbo' in '/earrmaer/dbo:rektae'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-prison",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
