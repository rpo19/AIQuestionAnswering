{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "statewide-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy_dbpedia_spotlight\n",
    "import spacy\n",
    "import requests\n",
    "import flair\n",
    "from pathlib import Path\n",
    "flair.cache_root = Path('../../data/flair')\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "from flair.embeddings import SentenceTransformerDocumentEmbeddings\n",
    "from scipy import spatial\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "\n",
    "class DBPediaEntityExtractor():\n",
    "    def __init__(self, mode='spotlight'):\n",
    "        self.mode = mode\n",
    "\n",
    "        if mode == 'spotlight':\n",
    "            # load model and keep only ner\n",
    "            print('Loading \\'en_core_web_lg\\' model...')\n",
    "            self.nlp = spacy.load('en_core_web_lg', \n",
    "                            disable=['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer'])\n",
    "            # add dbpedia-spotlight stage\n",
    "            self.nlp.add_pipe('dbpedia_spotlight', config={'overwrite_ents': True})\n",
    "        elif mode == 'custom':\n",
    "            print('Loading flair NER models...')\n",
    "            # load NER model\n",
    "            self.tagger = SequenceTagger.load('ner-fast')\n",
    "            # load sentence embedding model\n",
    "            self.embedding = SentenceTransformerDocumentEmbeddings('bert-base-nli-mean-tokens')\n",
    "    \n",
    "    \"\"\"\n",
    "    Get text sentence level embedding.\n",
    "    \"\"\"\n",
    "    def __get_text_embedding(self, text):\n",
    "        sentence = Sentence(text)\n",
    "        self.embedding.embed(sentence)\n",
    "        return sentence.embedding.tolist()\n",
    "    \n",
    "    \"\"\"\n",
    "    Extract entities from text.\n",
    "    \"\"\"\n",
    "    def __extract_entities(self, text):\n",
    "        sentence = Sentence(text)\n",
    "        self.tagger.predict(sentence)\n",
    "        \n",
    "        entities = sentence.to_dict(tag_type='ner')['entities']\n",
    "        entities = [entity['text'] for entity in entities]\n",
    "        return entities\n",
    "    \n",
    "    \"\"\"\n",
    "    Create tuples from a list with overlapping elements.\n",
    "    (e.g: [1,2,3] -> [(1,2), (2,3)])\n",
    "    \"\"\"\n",
    "    def __split_overlap(self, seq, size, overlap):\n",
    "        return [x for x in zip(*[seq[i::size-overlap] for i in range(size)])]\n",
    "\n",
    "    \"\"\"\n",
    "    Extend entity phrase with its neighbour.\n",
    "    \"\"\"\n",
    "    def __extend_entity(self, text, phr, max_len):\n",
    "        tmp_text = text.replace(phr, 'ENTITY')\n",
    "        # get question tokens\n",
    "        text_tokens = tmp_text.split()\n",
    "        # get position of current entity\n",
    "        index = text_tokens.index('ENTITY')\n",
    "\n",
    "        extended_entities = []\n",
    "\n",
    "        for size in range(1, max_len+1):\n",
    "            for group in self.__split_overlap(text_tokens, size, size-1):\n",
    "                print(group)\n",
    "                if 'ENTITY' in group:\n",
    "                    extended_entities.append(' '.join(group).replace('ENTITY', phr))\n",
    "        return extended_entities\n",
    "    \n",
    "    \"\"\"\n",
    "    Get query lookup results.\n",
    "    \"\"\"\n",
    "    def __lookup(self, phr, max_res = 10):\n",
    "        res = requests.get(f'https://lookup.dbpedia.org/api/search?query={phr}&maxResults=10&format=JSON_RAW')\n",
    "        docs = eval(res.text)['docs']\n",
    "        return docs\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute relevance between entity phrase and candidate entity.\n",
    "    score = alfa1 * importance + alfa2 * lev_distance + alfa3 * cos_sim\n",
    "    \"\"\"\n",
    "    def __compute_relevance(self, phr, candidate_entity, text_embedding, rank, alfa1=1, alfa2=1, alfa3=1):\n",
    "        # TODO: compute importance\n",
    "        # can we use the relevance or simply the rank of results from lookup?\n",
    "        importance = 1 / rank\n",
    "        \n",
    "        # compute lev distance\n",
    "        lev_distance = 1 / (levenshtein_distance(phr, candidate_entity['label'][0]) + 1)\n",
    "        \n",
    "        # compute relevance with doc embedding\n",
    "        if 'comment' in candidate_entity:\n",
    "            doc__entity_embedding = self.__get_text_embedding(candidate_entity['comment'])\n",
    "            cos_sim = 1 - spatial.distance.cosine(text_embedding, doc__entity_embedding)\n",
    "        else:\n",
    "            cos_sim = 0\n",
    "\n",
    "        score = alfa1 * importance + alfa2 * lev_distance + alfa3 * cos_sim\n",
    "        return score\n",
    "    \n",
    "    \"\"\"\n",
    "    Extract and link entities from a text as described from the paper.\n",
    "    \"\"\"\n",
    "    def __extract_custom(self, text, max_len = 3):\n",
    "    \n",
    "        entities_URIs = []\n",
    "        entities_texts = []\n",
    "        entities_scores = []\n",
    "        \n",
    "        # get text embedding\n",
    "        text_embedding = self.__get_text_embedding(text)\n",
    "        \n",
    "        # extract entities from question\n",
    "        entity_phrases = self.__extract_entities(text)\n",
    "        \n",
    "        # iterate for each extracted entity\n",
    "        for i, phr in enumerate(entity_phrases):\n",
    "            candidate_entity_phrase = {'phr': phr, 'candidate_entity': None, 'score': 0}\n",
    "            \n",
    "            # extend extracted entities\n",
    "            PX = self.__extend_entity(text, phr, max_len)\n",
    "            EC = []\n",
    "            ranks = []\n",
    "            # look for candidate entities\n",
    "            for phr_ext in PX:\n",
    "                docs = self.__lookup(phr_ext)\n",
    "                # if there is at least a match add to candidate entities\n",
    "                if len(docs) > 0:\n",
    "                    EC.extend(docs)\n",
    "                    ranks.extend(list(range(1, len(docs) + 1)))\n",
    "            # compute relevances and keep highest relevance candidate entity\n",
    "            for j, candidate_entity in enumerate(EC):\n",
    "                tmp_score = self.__compute_relevance(phr, candidate_entity, text_embedding, ranks[j])\n",
    "                if tmp_score > candidate_entity_phrase['score']:\n",
    "                    candidate_entity_phrase['candidate_entity'] = candidate_entity\n",
    "                    candidate_entity_phrase['score'] = tmp_score\n",
    "            \n",
    "            entities_URIs.append('<'+candidate_entity_phrase['candidate_entity']['resource'][0]+'>')\n",
    "            entities_texts.append(candidate_entity_phrase['phr'])\n",
    "            entities_scores.append(candidate_entity_phrase['score'])\n",
    "                \n",
    "        return entities_URIs, entities_texts, entities_scores\n",
    "\n",
    "    \"\"\"\n",
    "    Extract and link entities from a text with DBPedia Spotlight.\n",
    "    \"\"\"\n",
    "    def __spotlight_extract(self, text):\n",
    "        # execute NER and NEL\n",
    "        doc = self.nlp(text)\n",
    "        nel_ents = doc.ents\n",
    "        \n",
    "        # filter entities\n",
    "        filtered_ents_uri = []\n",
    "        filtered_ents_text = []\n",
    "        for nel_ent in nel_ents:\n",
    "            # if there are NER ents\n",
    "            try:\n",
    "                ner_ents = doc.spans['ents_original']\n",
    "                for ner_ent in ner_ents:\n",
    "                    # keep only entities extracted with both spacy's NER and dbpedia-spotlight\n",
    "                    if ner_ent.text == nel_ent.text:\n",
    "                        ent = {\n",
    "                            'id': nel_ent.kb_id_,\n",
    "                            'text': nel_ent.text\n",
    "                        }\n",
    "                        filtered_ents.append()\n",
    "            except:\n",
    "                # no NER ents, keep all the dbpedia-spotlight ones\n",
    "                filtered_ents_uri.append('<'+nel_ent.kb_id_+'>')\n",
    "                filtered_ents_text.append(nel_ent.text)\n",
    "        \n",
    "        return filtered_ents_uri, filtered_ents_text\n",
    "    \n",
    "    def extract(self, text):\n",
    "        if self.mode == 'spotlight':\n",
    "            return self.__spotlight_extract(text)\n",
    "        elif self.mode == 'custom':\n",
    "            return self.__extract_custom(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "rocky-wonder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading flair NER models...\n",
      "2021-04-25 12:14:33,981 --------------------------------------------------------------------------------\n",
      "2021-04-25 12:14:33,982 The model key 'ner-fast' now maps to 'https://huggingface.co/flair/ner-english-fast' on the HuggingFace ModelHub\n",
      "2021-04-25 12:14:33,982  - The most current version of the model is automatically downloaded from there.\n",
      "2021-04-25 12:14:33,983  - (you can alternatively manually download the original model at https://nlp.informatik.hu-berlin.de/resources/models/ner-fast/en-ner-fast-conll03-v0.4.pt)\n",
      "2021-04-25 12:14:33,984 --------------------------------------------------------------------------------\n",
      "2021-04-25 12:14:34,336 loading file ..\\..\\data\\flair\\models\\ner-english-fast\\4c58e7191ff952c030b82db25b3694b58800b0e722ff15427f527e1631ed6142.e13c7c4664ffe2bbfa8f1f5375bd0dced866b8c1dd7ff89a6d705518abf0a611\n"
     ]
    }
   ],
   "source": [
    "extractor = DBPediaEntityExtractor(mode='custom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "unsigned-bhutan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('who',)\n",
      "('is',)\n",
      "('the',)\n",
      "('wife',)\n",
      "('of',)\n",
      "('ENTITY',)\n",
      "('?',)\n",
      "('who', 'is')\n",
      "('is', 'the')\n",
      "('the', 'wife')\n",
      "('wife', 'of')\n",
      "('of', 'ENTITY')\n",
      "('ENTITY', '?')\n",
      "('who', 'is', 'the')\n",
      "('is', 'the', 'wife')\n",
      "('the', 'wife', 'of')\n",
      "('wife', 'of', 'ENTITY')\n",
      "('of', 'ENTITY', '?')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['<http://dbpedia.org/resource/Barack_Obama>'],\n",
       " ['Obama'],\n",
       " [1.5885680423912993])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor.extract('who is the wife of Obama ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-seeker",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
